# -*- coding: utf-8 -*-
"""C√≥pia de Assistente de Voz Multi-Idiomas Com Whisper e ChatGPT

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FsJJTIcsilxXWy8hI9bykvvq2rbb_wSC
"""

language = 'pt'

"""# 1. Grava√ß√£o de √Åudio Com Python (e Uma Pitada de JavaScript) üé§"""

# Refer√™ncia: https://gist.github.com/korakot/c21c3476c024ad6d56d5f48b0bca92be

from IPython.display import Audio, display, Javascript
from google.colab import output
from base64 import b64decode

# C√≥digo JavaScript para gravar √°udio do usu√°rio usando a "MediaStream Recording API"
RECORD = """
const sleep  = time => new Promise(resolve => setTimeout(resolve, time))
const b2text = blob => new Promise(resolve => {
  const reader = new FileReader()
  reader.onloadend = e => resolve(e.srcElement.result)
  reader.readAsDataURL(blob)
})
var record = time => new Promise(async resolve => {
  stream = await navigator.mediaDevices.getUserMedia({ audio: true })
  recorder = new MediaRecorder(stream)
  chunks = []
  recorder.ondataavailable = e => chunks.push(e.data)
  recorder.start()
  await sleep(time)
  recorder.onstop = async ()=>{
    blob = new Blob(chunks)
    text = await b2text(blob)
    resolve(text)
  }
  recorder.stop()
})
"""

def record(sec=5):
  # Executa o c√≥digo JavaScript para gravar o √°udio
  display(Javascript(RECORD))
  # Recebe o √°udio gravado como resultado do JavaScript
  js_result = output.eval_js('record(%s)' % (sec * 1000))
   # Decodifica o √°udio em base64
  audio = b64decode(js_result.split(',')[1])
  # Salva o √°udio em um arquivo
  file_name = 'request_audio.wav'
  with open(file_name, 'wb') as f:
    f.write(audio)
  # Retorna o caminho do arquivo de √°udio (pasta padr√£o do Google Colab)
  return f'/content/{file_name}'

# Grava o √°udio do usu√°rio por um tempo determinado (padr√£o 5 segundos)
print('Ouvindo...\n')
record_file = record()

# Exibe o √°udio gravado
display(Audio(record_file, autoplay=False))

"""# 2. Reconhecimento de Fala com Whisper (OpenAI) üß†"""

!pip install git+https://github.com/openai/whisper.git -q

import whisper

# Selecione o modelo do Whisper que melhor atenda √†s suas necessidades:
# https://github.com/openai/whisper#available-models-and-languages
model = whisper.load_model("small")

# Transcreve o audio gravado anteriormente.
result = model.transcribe(record_file, fp16=False, language=language)
transcription = result["text"]
print(transcription)

"""# 3. Integra√ß√£o com a API do ChatGPT üí¨"""

!pip install openai

import os

# Documenta√ß√£o Oficial da API OpenAI: https://platform.openai.com/docs/api-reference/introduction
# Informa√ß√µes sobre o Per√≠odo Gratuito: https://help.openai.com/en/articles/4936830

# Para gerar uma API Key:
# 1. Crie uma conta na OpenAI
# 2. Acesse a se√ß√£o "API Keys"
# 3. Clique em "Create API Key"
# Link direto: https://platform.openai.com/account/api-keys

# Substitua o texto "TODO" por sua API Key da OpenAI, ela ser√° salva como uma vari√°vel de ambiente.
os.environ['OPENAI_API_KEY'] = 'TODO'

import openai



#Atualiza√ß√£o
!pip install --upgrade openai


import os
from openai import OpenAI  # <-- ESSA LINHA ESTAVA FALTANDO

# Defina sua chave (mas veja aviso abaixo)
os.environ["OPENAI_API_KEY"] = "Chave-GPT"

client = OpenAI()

response = client.chat.completions.create(
    model="gpt-4o-mini",
    messages=[
        {"role": "user", "content": "Explique o que √© Machine Learning."}
    ],
)

print(response.choices[0].message.content)

"""# 4. Sintetizando a Resposta do ChatGPT Como Voz (gTTS) üîä"""

!pip install gTTS

from gtts import gTTS
from IPython.display import Audio, display

# Texto de teste (substitui chatgpt_response por enquanto)
chatgpt_response = "Ol√°, este √© um teste de s√≠ntese de voz no Google Colab."

language = "pt"

# Cria objeto de voz
gtts_object = gTTS(text=chatgpt_response, lang=language, slow=False)

# Salva arquivo
response_audio = "/content/response_audio.mp3"
gtts_object.save(response_audio)

# Reproduz
display(Audio(response_audio, autoplay=True))